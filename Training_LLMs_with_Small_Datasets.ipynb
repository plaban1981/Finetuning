{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plaban1981/Finetuning/blob/main/Training_LLMs_with_Small_Datasets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training LLMs with Small Datasets\n",
        "1. Understanding Model Size\n",
        "1. Quantization\n",
        "1. Basic Fine Tuning\n",
        "1. Advanced Fine Tuning"
      ],
      "metadata": {
        "id": "txUrxcHpHd8l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding Model Size\n",
        "\n",
        "Llama 2:\n",
        "- 7b\n",
        "- 13b\n",
        "- 70b\n",
        "\n",
        "Each weight represented by 32-bits.\n",
        "- 8 bits per byte\n",
        "- 70b model\n",
        "=> 70b x 32 bits / (8 bits per byte) = 280 GB.\n",
        "\n",
        "Llama 7b:\n",
        "- 7b x 32 bits / 8 bits per byte = 28 GB.\n",
        "\n",
        "A100 Nvidia - 40GB, 80GB."
      ],
      "metadata": {
        "id": "407H-o1UIPje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization\n",
        "\n",
        "Instead of using 32-bits to represent a weight. You scale it to 4-bits.\n",
        "\n",
        "2^4 = 32\n",
        "2^32 = .....\n",
        "\n",
        "7b model:\n",
        "- 7b x 4 bits / 8 bits per byte = 3.5 GB."
      ],
      "metadata": {
        "id": "k72wWFaIJCrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning with QLoRa\n",
        "\n",
        "Quantized LoRa - means training with quantized weights (4-bit in this case).\n",
        "\n",
        "LoRa - Low Rank. Instead of updating all 7b weights, we'll just update the most important ones.\n",
        "\n",
        "transformers library (huggingFace developed) -> bitsandbytes (a quantization library supporting LoRa).\n",
        "\n",
        "PEFT - parameter efficient fine tuning, another way to say \"we only train some weights\". LoRa is a type of PEFT."
      ],
      "metadata": {
        "id": "rqkSH1QdKITz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Fine Tuning.\n",
        "- Prompt masking\n",
        "- End of sequence token\n",
        "\n",
        "Attention:\n",
        "-is the idea that the prediction of the next token depends on earlier tokens.\n",
        "\n",
        "[The][ quick][ brown][ fox][ jumped][...]\n",
        "[1]      [1]    [1]    [1]    [1]\n",
        "\n",
        "|<pad>|[The][ quick][ brown][ fox][ jumped][...]\n",
        "   [0]  [1]      [1]    [1]    [1]    [1]\n",
        "  \n",
        "Loss Masking:\n",
        "- selecting what token predictions to penalize.\n",
        "\n",
        "Inputs: [pad][The][ quick][ brown][ fox][ jumped]\n",
        "Predic: [Hi][boy][ brown][ fox][ jumped][ over]\n",
        "Actual: [The][ quick][ brown][ fox][ jumped][over]\n",
        "Losses:  [4] [5] [0.01]    [0.2]    [0.1]   [0.3]\n",
        "\n",
        "The loss mask determines which tokens to include in the loss calculation...\n",
        "\n",
        "|<pad>|[The][ quick][ brown][ fox][ jumped]\n",
        "  [0].   [1]. [1].      [1].  [1].     [1]\n",
        "\n",
        "  Multiply the loss mask by the losses\n",
        "  [0].   [5].  [0.01].   [0.02].  [0.1].  [0.3]\n",
        "\n"
      ],
      "metadata": {
        "id": "Eyue6mKAK2It"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  <s> [INST] <<SYS>>\n",
        "\n",
        "You are a helpful assistant that provides accurate and concise responses\n",
        "<</SYS>>\n",
        "\n",
        "Provide a very brief comparison of salsa and bachata. [/INST]\n",
        "\n",
        "{ function_call: }"
      ],
      "metadata": {
        "id": "pZsrHFIlIhzs"
      }
    }
  ]
}